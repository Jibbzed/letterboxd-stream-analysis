{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This notebook aims at applying the first stream processing technique to the dataset, namely a count of the distinct elements.\\\n",
    "We will be using the Flajolet-Martin algorithm to do so, with the goal to count the number of distinct actors in the actors.csv file.\\\n",
    "The Kaggle page tells us that the actual number of unique names is 1 513 888, value that we will be comparing to our result.\\\n",
    "The algorithm has to be re implemented from scracth, and 2 versions will be implemented, a simple version with only one hash function, and a refined version using multiple hash functions and aggregating their results.\\\n",
    "It will be applied to a downsampled version of the dataset by default, with the possibility to use the full dataset.\\\n",
    "We will compare the results with the successor of the Flajolet-Martin, namely HyperLogLog (HLL), that we will import from a library."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import the needed modules\n",
    "import csv\n",
    "import hashlib\n",
    "import random\n",
    "import sys"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the global variables\n",
    "real_count = 1513888\n",
    "sampled = False\n",
    "sr = 0.1 if sampled else 1.0\n",
    "phi = 0.77351"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Let's define a Flajolet-Martin class that encapsulates all the functions needed to perform the steps of the algorithm\n",
    "class FlajoletMartinSimple:\n",
    "    # Only thing we need to estimate the cardinality is the max number of leading zeros (tail length) of the hash values\n",
    "    def __init__(self):\n",
    "        self.max_tail_length = 0\n",
    "\n",
    "    # We need to use a hash function to hash the values, here we use SHA-256\n",
    "    # The output set of the hash function has to be bigger than the set of elements we are using, with sha256 we should be good\n",
    "    def _hash(self, value):\n",
    "        hash_value = hashlib.sha256(value.encode('utf8')).hexdigest()\n",
    "        return int(hash_value, 16)\n",
    "\n",
    "    # We need the tail length of the binary representation of the hash value\n",
    "    # Note that we could use the number of leading zeros, but it is apparently not completely equivalent because of the distribution of the values\n",
    "    def _tail_length(self, hash_value):\n",
    "        binary_hash = bin(hash_value)[2:]  # Convert hash to binary string and remove '0b' prefix added automatically by the bin function\n",
    "        # We want the number of zeros at the end of the binary representation\n",
    "        return len(binary_hash) - len(binary_hash.rstrip('0'))\n",
    "\n",
    "    # For each value in the dataset we will process it, namely computing its hash, the tail length, and update the max value if needed\n",
    "    def process(self, value):\n",
    "        hash_value = self._hash(value)\n",
    "        tail_length = self._tail_length(hash_value)\n",
    "        self.max_tail_length = max(self.max_tail_length, tail_length)\n",
    "\n",
    "    # According to the Flajolet-Martin algorithm, the cardinality is estimated as 2^max_tail_length\n",
    "    def estimate_cardinality(self):\n",
    "        return 2 ** self.max_tail_length"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define a process csv function that will apply the algortihm to the stream of data\n",
    "def process_csv(file, fm, sample_rate=1.0):\n",
    "    with open(file, 'r', encoding='utf-8') as csvfile:\n",
    "        # We will make use of the DictReader class to read the file more easily (dictionary for each row, headers handled automatically)\n",
    "        csvreader = csv.DictReader(csvfile)\n",
    "        counter = 0\n",
    "        # We treat the file as a stream, processing each row independently\n",
    "        for row in csvreader:\n",
    "            # We use the sample rate to choose at random a certain fraction of the data\n",
    "            if random.random() <= sample_rate:\n",
    "                actor = row['name']\n",
    "                fm.process(actor)\n",
    "            counter += 1\n",
    "\n",
    "    return fm.estimate_cardinality(), counter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Estimated number of unique names : 1048576\n",
      "Total number of names processed : 5523327\n",
      "Real number of unique names : 1513888\n",
      "Relative error : 30.74%\n"
     ]
    }
   ],
   "source": [
    "# Now we can use everything we defined to process our data\n",
    "fm = FlajoletMartinSimple()\n",
    "cardinality, count = process_csv('data/actors.csv', fm, sample_rate=sr)\n",
    "print(f\"Estimated number of unique names : {cardinality}\")\n",
    "print(f\"Total number of names processed : {count}\")\n",
    "if not sampled:\n",
    "    print(f\"Real number of unique names : {real_count}\")\n",
    "    error = abs(cardinality - real_count) / real_count\n",
    "    print(f\"Relative error : {error:.2%}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "With the simple Flajolet-Martin algorithm, we can process all 5.5 million elements in the actors file in less than 30 seconds, and we get an estimate with a relative error of 30%.\\\n",
    "After diving a bit more into the paper, it turns out that the FM algorithm introduces a correction factor $\\phi$ to account for bias. The exact calculations are detailed in the original paper, but we get that $\\phi = 0.77351$, and the estimate is $2^r/\\phi$. Dividing by a factor of 0.7 means an increase of about 30%, which corresponds to our relative error. If we introduce the correction, we should see the basic FM algorithm produce quite good results !"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Estimated number of unique names : 1355607\n",
      "Total number of names processed : 5523327\n",
      "Real number of unique names : 1513888\n",
      "Relative error : 10.46%\n"
     ]
    }
   ],
   "source": [
    "# Let's try to re run the algorithm, this time applying the correction factor\n",
    "cardinality, count = process_csv('data/actors.csv', fm, sample_rate=sr)\n",
    "cardinality = int(cardinality / phi)\n",
    "print(f\"Estimated number of unique names : {cardinality}\")\n",
    "print(f\"Total number of names processed : {count}\")\n",
    "if not sampled:\n",
    "    print(f\"Real number of unique names : {real_count}\")\n",
    "    error = abs(cardinality - real_count) / real_count\n",
    "    print(f\"Relative error : {error:.2%}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As expected, we get a way better result if we account for the correction factor (in that instance, 10% instead of 30) !\\\n",
    "But the results can still be improved, especially by using several hash functions and aggregating the data."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As we have seen, the mean is not a very good aggregator because it is very sensitive to outliers.\\\n",
    "A better aggregator is the median, but it has a problem of granularity, namely that the value is one of the values in the set, so a value of the form 2^r, which is not that amazing in terms of granularity.\\\n",
    "A good aggregator would therefore be to use a hybrid approach : for k*l hash functions, split the functions in l groups of k, apply the algorithm, aggregate the results with the median in each group, then aggregate the groups using the mean."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Let's define a new class that will use multiple hash functions to improve the accuracy of the estimation\n",
    "class FlajoletMartinMultiple:\n",
    "    # This time, we will use multiple hash functions, and we will keep the max tail length for each of them\n",
    "    # Note that the multiple hash functions are not different schemes, but the same scheme with different seeds (still using SHA-256)\n",
    "    def __init__(self, num_groups=4, hashes_per_group=5):\n",
    "        self.num_groups = num_groups\n",
    "        self.hashes_per_group = hashes_per_group\n",
    "        self.num_hashes = num_groups * hashes_per_group\n",
    "        self.max_tail_length = [[0] * hashes_per_group for _ in range(num_groups)]\n",
    "        self.seeds = [random.randint(0, 2**32 - 1) for _ in range(self.num_hashes)]\n",
    "        self.phi = phi  # Correction factor (this time we include it directly in the class)\n",
    "\n",
    "    # The seed is used to artificially create different hash functions\n",
    "    # It is not used directly in the hash function but concatenated with the value to be hashed\n",
    "    def _hash(self, value, seed):\n",
    "        hash_input = f\"{seed}-{value}\"\n",
    "        hash_value = hashlib.sha256(hash_input.encode('utf8')).hexdigest()\n",
    "        return int(hash_value, 16)\n",
    "\n",
    "    # The tail length is computed as before\n",
    "    def _tail_length(self, hash_value):\n",
    "        binary_hash = bin(hash_value)[2:]\n",
    "        return len(binary_hash) - len(binary_hash.rstrip('0'))\n",
    "\n",
    "    # The process function is basically the same except that we process the value num_hash times\n",
    "    def process(self, value):\n",
    "        for group in range(self.num_groups):\n",
    "            for i in range(self.hashes_per_group):\n",
    "                # We use the index to get the right seed for the hash function (the seeds are kept in a one dimensional array for simplicity)\n",
    "                index = group * self.hashes_per_group + i\n",
    "                hash_value = self._hash(value, self.seeds[index])\n",
    "                trailing_zeros = self._tail_length(hash_value)\n",
    "                self.max_tail_length[group][i] = max(self.max_tail_length[group][i], trailing_zeros)\n",
    "\n",
    "    # The estimate cardinality is now computed as the median of the estimates of each hash function\n",
    "    def estimate_cardinality(self):\n",
    "        # Compute the median of each group\n",
    "        group_medians = []\n",
    "        for group in self.max_tail_length:\n",
    "            # Apply the correction directly here\n",
    "            sorted_array = sorted([2 ** r / self.phi for r in group])\n",
    "            median = sorted_array[len(sorted_array) // 2]\n",
    "            group_medians.append(median)\n",
    "            print(sorted_array)\n",
    "        # Then compute the mean of the medians\n",
    "        print(group_medians)\n",
    "        estimate = sum(group_medians) / len(group_medians)\n",
    "        return int(estimate)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[338901.88879264647, 677803.7775852929, 677803.7775852929, 1355607.5551705859, 2711215.1103411717, 5422430.2206823435, 5422430.2206823435]\n",
      "[338901.88879264647, 677803.7775852929, 677803.7775852929, 1355607.5551705859, 5422430.2206823435, 10844860.441364687, 86758883.5309175]\n",
      "[338901.88879264647, 677803.7775852929, 1355607.5551705859, 2711215.1103411717, 2711215.1103411717, 5422430.2206823435, 43379441.76545875]\n",
      "[677803.7775852929, 1355607.5551705859, 2711215.1103411717, 2711215.1103411717, 5422430.2206823435, 5422430.2206823435, 43379441.76545875]\n",
      "[338901.88879264647, 677803.7775852929, 677803.7775852929, 677803.7775852929, 2711215.1103411717, 2711215.1103411717, 5422430.2206823435]\n",
      "[1355607.5551705859, 1355607.5551705859, 2711215.1103411717, 2711215.1103411717, 677803.7775852929]\n",
      "Estimated number of unique names : 1762289\n",
      "Total number of names processed : 5523327\n",
      "Real number of unique names : 1513888\n",
      "Relative error : 16.41%\n"
     ]
    }
   ],
   "source": [
    "# The process csv function is the same, we just need to change the class used\n",
    "fm = FlajoletMartinMultiple(5,7)\n",
    "cardinality, count = process_csv('data/actors.csv', fm, sample_rate=sr)\n",
    "print(f\"Estimated number of unique names : {cardinality}\")\n",
    "print(f\"Total number of names processed : {count}\")\n",
    "if not sampled:\n",
    "    print(f\"Real number of unique names : {real_count}\")\n",
    "    error = abs(cardinality - real_count) / real_count\n",
    "    print(f\"Relative error : {error:.2%}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This time, as expected, the processing time for the full dataset is way larger, simply because we are processing each row n times instead of 1 ! The default case is 4 groups of 5 hash functions, so the processing would take about 20 times longer, which can be a lot, but is still reasonable considering that the simple method takes less than 30 seconds. In this instance, aka 3 groups of 7 hashes, it takes around 6 minutes to process all the data.\\\n",
    "For some reason, the results are worse with this method, some combinations of number of groups / number of hashes per group seem to yield better results but nothing really amazing.\n",
    "There might be some problem somewhere in the implementation, maybe the use of the seed is wrong and I should be using a set of completely diverse hash functions (like MD5, SHA1, ...) ?\n",
    "We will try to either initialize a set of hash functions without using random seeds, for example concatenating 1, 2, 3, ... to the value and see if that makes a difference, and also to use a diversity of hash functions and see the difference."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To have an exact approach we could use a set, which would be faster in terms of processing time, but would not scale at all, simply because we would have to maintain a set of all the unique names that come up, and that set could be impossible to store in main memory in the case of a very large dataset.\\\n",
    "We will now be quickly comparing this FM algorithm, with its successor called the HyperLogLog (HLL). This method however, will not be re implemented from scratch but imported from a library, given that the goal is just to compare the accuracy.\\\n",
    "It is said that \"The HyperLogLog algorithm is able to estimate cardinalities of > $10^9$ with a typical accuracy (standard error) of 2%, using 1.5 kB of memory\". This is much better than the estimate we obtained, but let's see it in action."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from datasketch import HyperLogLog\n",
    "\n",
    "def process_csv_hll(file, hll, sample_rate=1.0):\n",
    "    with open(file, 'r', encoding='utf-8') as csvfile:\n",
    "        csvreader = csv.DictReader(csvfile)\n",
    "        counter = 0\n",
    "        for row in csvreader:\n",
    "            if random.random() <= sample_rate:\n",
    "                actor = row['actor']\n",
    "                hll.update(actor.encode('utf8'))\n",
    "            counter += 1\n",
    "\n",
    "    return hll.count(), counter\n",
    "\n",
    "# Initialize the HyperLogLog object\n",
    "# This algorithm allows us to set a precision parameter, in our case we will use a 2% error rate\n",
    "hll = HyperLogLog(0.02)\n",
    "\n",
    "# Process the data\n",
    "cardinality, count = process_csv_hll('data/actors.csv', hll, sample_rate=sr)\n",
    "print(f\"Estimated number of unique names : {cardinality}\")\n",
    "print(f\"Total number of names processed : {count}\")\n",
    "if not sampled:\n",
    "    print(f\"Real number of unique names : {real_count}\")\n",
    "    error = abs(cardinality - real_count) / real_count\n",
    "    print(f\"Relative error : {error:.2%}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In that case, since the HLL algorithm allows us to set the precision ourselves, it is not very interesting to compare the actual precision since we know what it will be. We can however, compare the execution time and the memory usage of both algorithm to get an idea of how effective the FM algorithm is.\\\n",
    "The memory usage of the FM algorithm can be easily computed : we need to maintain an array of number of groups * hashes per group integers, therefore the memory usage is groups * hashes * size(int).\\\n",
    "For the HLL algorithm it is a bit more delicate. In fact, the memory usage depends on the precision the user wants : the algorithm works by maintaining an array of registers, and the more precision, the more registers, the more memory is needed. But this is not too hard to compute and we can quite easily write a function to do so for us."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Memory usage of the FM algorithm\n",
    "# We make use of the sys library to get the size of an integer on this machine\n",
    "mem = fm.num_groups * fm.hashes_per_group * sys.getsizeof(0)\n",
    "print(f\"Memory usage of the Flajolet-Martin algorithm : {mem} bytes\")\n",
    "\n",
    "# Memory usage of the HyperLogLog algorithm\n",
    "\n",
    "def memory_usage_hll(hll):\n",
    "    # The memory usage is computed as the sum of the memory usage of the registers\n",
    "    # Each register is a 8 bytes integer\n",
    "    return hll.size() * 8"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Mechanism of HLL Algorithm\n",
    "\n",
    "Registers:\n",
    "The HLL algorithm uses a fixed array of registers to store information about the hashed values of the elements being counted.\n",
    "The number of registers $m$ is a power of 2, and it determines the precision of the algorithm. Specifically, $m=2^b$, where $b$ is the number of bits used to index the registers.\n",
    "\n",
    "Error Rate:\n",
    "The standard error rate $\\epsilon$ of the HLL algorithm is given by $\\epsilon \\approx 1.04/\\sqrt{m}$.\n",
    "Therefore, to achieve a desired error rate $\\epsilon$, you need to set the number of registers $m$ such that $m=(1.04/\\epsilon)^2$.\n",
    "This relationship shows that increasing the number of registers decreases the error rate (i.e., improves accuracy).\n",
    "\n",
    "Hashing:\n",
    "Each incoming element is hashed to a large binary number. The first $b$ bits of the hash value determine which register to update.\n",
    "The remaining bits are used to count the number of leading zeros, which is stored in the selected register.\n",
    "\n",
    "Estimation:\n",
    "The algorithm estimates the cardinality by combining the information stored in all registers. The harmonic mean of the register values is used to compute the raw estimate, which is then corrected using empirical constants."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
