{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This notebook aims at applying the first stream processing technique to the dataset, namely a count of the distinct elements.\\\n",
    "We will be using the Flajolet-Martin algorithm to do so, with the goal to count the number of distinct actors in the actors.csv file.\\\n",
    "The Kaggle page tells us that the actual number of unique names is 1 513 888, value that we will be comparing to our result.\\\n",
    "The algorithm has to be re implemented from scracth, and 2 versions will be implemented, a simple version with only one hash function, and a refined version using multiple hash functions and aggregating their results.\\\n",
    "It will be applied to a downsampled version of the dataset by default, with the possibility to use the full dataset.\\\n",
    "We will compare the results with the successor of the Flajolet-Martin, namely HyperLogLog (HLL), that we will import from a library."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import the needed modules\n",
    "import csv\n",
    "import hashlib\n",
    "import random\n",
    "import sys\n",
    "import mmh3\n",
    "import math"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the global variables\n",
    "real_count = 1513888\n",
    "sampled = False\n",
    "sr = 0.1 if sampled else 1.0\n",
    "phi = 0.77351"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Let's define a Flajolet-Martin class that encapsulates all the functions needed to perform the steps of the algorithm\n",
    "class FlajoletMartinSimple:\n",
    "    # Only thing we need to estimate the cardinality is the max number of leading zeros (tail length) of the hash values\n",
    "    def __init__(self):\n",
    "        self.max_tail_length = 0\n",
    "\n",
    "    # We need to use a hash function to hash the values, here we use SHA-256\n",
    "    # The output set of the hash function has to be bigger than the set of elements we are using, with sha256 we should be good\n",
    "    def _hash(self, value):\n",
    "        hash_value = hashlib.sha256(value.encode('utf8')).hexdigest()\n",
    "        return int(hash_value, 16)\n",
    "\n",
    "    # We need the tail length of the binary representation of the hash value\n",
    "    # Note that we could use the number of leading zeros, but it is apparently not completely equivalent because of the distribution of the values\n",
    "    def _tail_length(self, hash_value):\n",
    "        binary_hash = bin(hash_value)[2:]  # Convert hash to binary string and remove '0b' prefix added automatically by the bin function\n",
    "        # We want the number of zeros at the end of the binary representation\n",
    "        return len(binary_hash) - len(binary_hash.rstrip('0'))\n",
    "\n",
    "    # For each value in the dataset we will process it, namely computing its hash, the tail length, and update the max value if needed\n",
    "    def process(self, value):\n",
    "        hash_value = self._hash(value)\n",
    "        tail_length = self._tail_length(hash_value)\n",
    "        self.max_tail_length = max(self.max_tail_length, tail_length)\n",
    "\n",
    "    # According to the Flajolet-Martin algorithm, the cardinality is estimated as 2^max_tail_length\n",
    "    def estimate_cardinality(self):\n",
    "        return 2 ** self.max_tail_length"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define a process csv function that will apply the algortihm to the stream of data\n",
    "def process_csv(file, fm, sample_rate=1.0):\n",
    "    with open(file, 'r', encoding='utf-8') as csvfile:\n",
    "        # We will make use of the DictReader class to read the file more easily (dictionary for each row, headers handled automatically)\n",
    "        csvreader = csv.DictReader(csvfile)\n",
    "        counter = 0\n",
    "        # We treat the file as a stream, processing each row independently\n",
    "        for row in csvreader:\n",
    "            # We use the sample rate to choose at random a certain fraction of the data\n",
    "            if random.random() <= sample_rate:\n",
    "                actor = row['name']\n",
    "                fm.process(actor)\n",
    "            counter += 1\n",
    "\n",
    "    return fm.estimate_cardinality(), counter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Estimated number of unique names : 1048576\n",
      "Total number of names processed : 5523327\n",
      "Real number of unique names : 1513888\n",
      "Relative error : 30.74%\n"
     ]
    }
   ],
   "source": [
    "# Now we can use everything we defined to process our data\n",
    "fm = FlajoletMartinSimple()\n",
    "cardinality, count = process_csv('data/actors.csv', fm, sample_rate=sr)\n",
    "print(f\"Estimated number of unique names : {cardinality}\")\n",
    "print(f\"Total number of names processed : {count}\")\n",
    "if not sampled:\n",
    "    print(f\"Real number of unique names : {real_count}\")\n",
    "    error = abs(cardinality - real_count) / real_count\n",
    "    print(f\"Relative error : {error:.2%}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "With the simple Flajolet-Martin algorithm, we can process all 5.5 million elements in the actors file in less than 30 seconds, and we get an estimate with a relative error of 30%.\\\n",
    "After diving a bit more into the paper, it turns out that the FM algorithm introduces a correction factor $\\phi$ to account for bias. The exact calculations are detailed in the original paper, but we get that $\\phi = 0.77351$, and the estimate is $2^r/\\phi$. Dividing by a factor of 0.7 means an increase of about 30%, which corresponds to our relative error. If we introduce the correction, we should see the basic FM algorithm produce quite good results !"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Estimated number of unique names : 1355607\n",
      "Total number of names processed : 5523327\n",
      "Real number of unique names : 1513888\n",
      "Relative error : 10.46%\n"
     ]
    }
   ],
   "source": [
    "# Let's try to re run the algorithm, this time applying the correction factor\n",
    "cardinality, count = process_csv('data/actors.csv', fm, sample_rate=sr)\n",
    "cardinality = int(cardinality / phi)\n",
    "print(f\"Estimated number of unique names : {cardinality}\")\n",
    "print(f\"Total number of names processed : {count}\")\n",
    "if not sampled:\n",
    "    print(f\"Real number of unique names : {real_count}\")\n",
    "    error = abs(cardinality - real_count) / real_count\n",
    "    print(f\"Relative error : {error:.2%}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As expected, we get a way better result if we account for the correction factor (in that instance, 10% instead of 30) !\\\n",
    "But the results can still be improved, especially by using several hash functions and aggregating the data."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As we have seen, the mean is not a very good aggregator because it is very sensitive to outliers.\\\n",
    "A better aggregator is the median, but it has a problem of granularity, namely that the value is one of the values in the set, so a value of the form $2^r$, which is not that amazing in terms of granularity.\\\n",
    "A good aggregator would therefore be to use a hybrid approach : for $k\\times l$ hash functions, split the functions in $l$ groups of $k$, apply the algorithm, aggregate the results with the median in each group, then aggregate the groups using the mean."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Let's define a new class that will use multiple hash functions to improve the accuracy of the estimation\n",
    "class FlajoletMartinMultiple:\n",
    "    # This time, we will use multiple hash functions, and we will keep the max tail length for each of them\n",
    "    # Note that the multiple hash functions are not different schemes, but the same scheme with different seeds (still using SHA-256)\n",
    "    def __init__(self, num_groups=4, hashes_per_group=5):\n",
    "        self.num_groups = num_groups\n",
    "        self.hashes_per_group = hashes_per_group\n",
    "        self.num_hashes = num_groups * hashes_per_group\n",
    "        self.max_tail_length = [[0] * hashes_per_group for _ in range(num_groups)]\n",
    "        self.seeds = [random.randint(0, 2**32 - 1) for _ in range(self.num_hashes)]\n",
    "        self.phi = phi  # Correction factor (this time we include it directly in the class)\n",
    "\n",
    "    # We will be using the MMH3 (MurMurHash3) hash function, which is a set of fast and robust hash functions and should solve the performance problem\n",
    "    # that we had when working with only SHA-256 and using seed concatenated with the value to be hashed\n",
    "    def _hash(self, value, seed):\n",
    "        hash_value = mmh3.hash128(value, seed)\n",
    "        return hash_value\n",
    "\n",
    "    # The tail length is computed as before\n",
    "    def _tail_length(self, hash_value):\n",
    "        binary_hash = bin(hash_value)[2:]\n",
    "        return len(binary_hash) - len(binary_hash.rstrip('0'))\n",
    "\n",
    "    # The process function is basically the same except that we process the value num_hash times\n",
    "    def process(self, value):\n",
    "        for group in range(self.num_groups):\n",
    "            for i in range(self.hashes_per_group):\n",
    "                # We use the index to get the right seed for the hash function (the seeds are kept in a one dimensional array for simplicity)\n",
    "                index = group * self.hashes_per_group + i\n",
    "                hash_value = self._hash(value, self.seeds[index])\n",
    "                trailing_zeros = self._tail_length(hash_value)\n",
    "                self.max_tail_length[group][i] = max(self.max_tail_length[group][i], trailing_zeros)\n",
    "\n",
    "    # The estimate cardinality is now computed as the median of the estimates of each hash function\n",
    "    def estimate_cardinality(self):\n",
    "        # Compute the median of each group\n",
    "        group_medians = []\n",
    "        for group in self.max_tail_length:\n",
    "            # Apply the correction directly here\n",
    "            sorted_array = sorted([2 ** r / self.phi for r in group])\n",
    "            median = sorted_array[len(sorted_array) // 2]\n",
    "            group_medians.append(median)\n",
    "            print(sorted_array)\n",
    "        # Then compute the mean of the medians\n",
    "        print(group_medians)\n",
    "        estimate = sum(group_medians) / len(group_medians)\n",
    "        return int(estimate)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[677803.7775852929, 1355607.5551705859, 1355607.5551705859, 1355607.5551705859, 5422430.2206823435, 21689720.882729374, 21689720.882729374]\n",
      "[338901.88879264647, 338901.88879264647, 677803.7775852929, 1355607.5551705859, 1355607.5551705859, 2711215.1103411717, 5422430.2206823435]\n",
      "[677803.7775852929, 677803.7775852929, 1355607.5551705859, 1355607.5551705859, 1355607.5551705859, 21689720.882729374, 43379441.76545875]\n",
      "[338901.88879264647, 677803.7775852929, 677803.7775852929, 1355607.5551705859, 2711215.1103411717, 5422430.2206823435, 21689720.882729374]\n",
      "[338901.88879264647, 677803.7775852929, 1355607.5551705859, 2711215.1103411717, 2711215.1103411717, 21689720.882729374, 43379441.76545875]\n",
      "[1355607.5551705859, 1355607.5551705859, 1355607.5551705859, 1355607.5551705859, 2711215.1103411717]\n",
      "Estimated number of unique names : 1626729\n",
      "Total number of names processed : 5523327\n",
      "Real number of unique names : 1513888\n",
      "Relative error : 7.45%\n"
     ]
    }
   ],
   "source": [
    "# The process csv function is the same, we just need to change the class used\n",
    "fm = FlajoletMartinMultiple(5,7)\n",
    "cardinality, count = process_csv('data/actors.csv', fm, sample_rate=sr)\n",
    "print(f\"Estimated number of unique names : {cardinality}\")\n",
    "print(f\"Total number of names processed : {count}\")\n",
    "if not sampled:\n",
    "    print(f\"Real number of unique names : {real_count}\")\n",
    "    error = abs(cardinality - real_count) / real_count\n",
    "    print(f\"Relative error : {error:.2%}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This time, as expected, the processing time for the full dataset is way larger, simply because we are processing each row n times instead of 1 ! The default case is 4 groups of 5 hash functions, so the processing would take about 20 times longer, which can be a lot, but is still reasonable considering that the simple method takes less than 30 seconds.\\\n",
    "Tried using the MurmurHash library. The mmh3 hash is faster than SHA256, leading to way faster execution time (4 minutes instead of about 10 with SHA for 35 hashes). The seeds can also be directly input in the function instead of concatenated with the value to be hashed.\\\n",
    "The results are better, but quite unstable it seems : 7.45% relative error on one execution, but sometimes way more, whereas the simple one hash version always produces around 10%.\\\n",
    "Increasing the number of groups and hashes per group should reduce variance but it also increases the computation time by a lot : at what point is it \"enough\" to actually observe a decrease in variance, and is this number actually usable or does it just take hours to execute ?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To have an exact approach we could use a set, which would be faster in terms of processing time, but would not scale at all, simply because we would have to maintain a set of all the unique names that come up, and that set could be impossible to store in main memory in the case of a very large dataset.\\\n",
    "We will now be quickly comparing this FM algorithm, with its successor called the HyperLogLog (HLL). This method however, will not be re implemented from scratch but imported from a library, given that the goal is just to compare the accuracy.\\\n",
    "It is said that \"The HyperLogLog algorithm is able to estimate cardinalities of > $10^9$ with a typical accuracy (standard error) of 2%, using 1.5 kB of memory\". This is much better than the estimate we obtained, but let's see it in action."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Estimated number of unique names : 1564700\n",
      "Total number of names processed : 5523327\n",
      "Real number of unique names : 1513888\n",
      "Relative error : 3.36%\n"
     ]
    }
   ],
   "source": [
    "import hyperloglog\n",
    "\n",
    "def process_csv_hll(file, hll, sample_rate=1.0):\n",
    "    with open(file, 'r', encoding='utf-8') as csvfile:\n",
    "        csvreader = csv.DictReader(csvfile)\n",
    "        counter = 0\n",
    "        for row in csvreader:\n",
    "            if random.random() <= sample_rate:\n",
    "                actor = row['name']\n",
    "                hll.add(actor)\n",
    "            counter += 1\n",
    "\n",
    "    return len(hll), counter\n",
    "\n",
    "# Initialize the HyperLogLog object\n",
    "# This algorithm allows us to set a precision parameter, in our case we will use a 2% error rate\n",
    "hll = hyperloglog.HyperLogLog(0.02)\n",
    "\n",
    "# Process the data\n",
    "cardinality, count = process_csv_hll('data/actors.csv', hll, sample_rate=sr)\n",
    "print(f\"Estimated number of unique names : {cardinality}\")\n",
    "print(f\"Total number of names processed : {count}\")\n",
    "if not sampled:\n",
    "    print(f\"Real number of unique names : {real_count}\")\n",
    "    error = abs(cardinality - real_count) / real_count\n",
    "    print(f\"Relative error : {error:.2%}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In that case, since the HLL algorithm allows us to set the precision ourselves, it is not very interesting to compare the actual precision since we know what it will be. We can however, compare the execution time and the memory usage of both algorithm to get an idea of how effective the FM algorithm is.\\\n",
    "With a precision set to 2%, we get a relative error of 3.36%, which is close enough, and the execution takes less than 30 seconds, so it is much faster than the FM algorithm (if we consider the refined version) and yields better results.\\\n",
    "The memory usage of the FM algorithm can be easily computed : we need to maintain an array of number of groups * hashes per group integers, therefore the memory usage is groups * hashes * size(int).\\\n",
    "For the HLL algorithm it is a bit more delicate. In fact, the memory usage depends on the precision the user wants : the algorithm works by maintaining an array of registers, and the more precision, the more registers, the more memory is needed. But this is not too hard to compute and we can quite easily write a function to do so for us."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Memory usage of the Flajolet-Martin algorithm : 980 bytes\n",
      "m = 4096, b = 12.0, w = 52.0\n",
      "Memory usage of the HyperLogLog algorithm : 2918 bytes\n"
     ]
    }
   ],
   "source": [
    "# Memory usage of the FM algorithm\n",
    "# We make use of the sys library to get the size of an integer on this machine\n",
    "mem = fm.num_groups * fm.hashes_per_group * sys.getsizeof(0)\n",
    "print(f\"Memory usage of the Flajolet-Martin algorithm : {mem} bytes\")\n",
    "\n",
    "def memory_usage_hll(hll):\n",
    "    # The memory usage is computed as the sum of the memory usage of the registers\n",
    "    # The registers store the maximum number of leading zeros for each hash value, therefore the size of the register depends on the hash function\n",
    "    # used (for example, 256 bits for SHA-256) and the parameters of the algorithm (how many bits are used to index the register)\n",
    "    # In the original paper, a 32 bits hash function was used, but the library uses HLL+ which uses 64 bits hash functions\n",
    "    # We will use the naming convention of the paper, m is the number of registers, b is the number of bits used to index the register, w is size_hash - b\n",
    "    # Since the number of registers is a power of 2, we can use the formula 2^b = m to get the number of bits needed to index the register\n",
    "    m = hll.m \n",
    "    b = math.log2(m)\n",
    "    w = 64 - b\n",
    "    print(f\"m = {m}, b = {b}, w = {w}\")\n",
    "    # Now we know that there can be at most w zeros in the hash value, i.e. the number of leading zeros is at most w, if the hash is full of zeros\n",
    "    # Therefore, we need to store log_2(w) bits to store the maximum number of leading zeros in each register\n",
    "    # Since we have m registers, the total memory usage is m * log_2(w) bits, or m * log_2(w) / 8 bytes\n",
    "    return int(m * math.log2(w) / 8)\n",
    "\n",
    "# Memory usage of the HyperLogLog algorithm\n",
    "mem = memory_usage_hll(hll)\n",
    "print(f\"Memory usage of the HyperLogLog algorithm : {mem} bytes\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can see that even though the HLL algorithm does not use a lot of memory, the FM algorithm is even more efficient in terms of memory.\\\n",
    "Note though, that we are not using that many hashes for our FM algorithm and that this probably costs us a bit of accuracy.\\\n",
    "The HLL algorithm provides better results in way less time, and with a memory consumption that is not that enormous, even though a bit higher than FM."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Mechanism of HLL Algorithm\n",
    "\n",
    "Registers:\n",
    "The HLL algorithm uses a fixed array of registers to store information about the hashed values of the elements being counted.\n",
    "The number of registers $m$ is a power of 2, and it determines the precision of the algorithm. Specifically, $m=2^b$, where $b$ is the number of bits used to index the registers.\n",
    "\n",
    "Error Rate:\n",
    "The standard error rate $\\epsilon$ of the HLL algorithm is given by $\\epsilon \\approx 1.04/\\sqrt{m}$.\n",
    "Therefore, to achieve a desired error rate $\\epsilon$, you need to set the number of registers $m$ such that $m=(1.04/\\epsilon)^2$.\n",
    "This relationship shows that increasing the number of registers decreases the error rate (i.e., improves accuracy).\n",
    "\n",
    "Hashing:\n",
    "Each incoming element is hashed to a large binary number. The first $b$ bits of the hash value determine which register to update.\n",
    "The remaining bits are used to count the number of leading zeros, which is stored in the selected register.\n",
    "\n",
    "Estimation:\n",
    "The algorithm estimates the cardinality by combining the information stored in all registers. The harmonic mean of the register values is used to compute the raw estimate, which is then corrected using empirical constants."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
