{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The goal of this notebook is to create a bloom filter on movie genres.\\\n",
    "The idea of this filter is to choose a genre, for instance western, and to create a filter on all the movies that have this genre. This can be done through 2 ways, either by using the movie ID, or using the full vector of informations about the movie (without the genre attribute).\\\n",
    "The goal is to be able to, without the genre information, be able to \"reconstruct\" the information using the filter."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import the needed modules as always\n",
    "import csv\n",
    "import mmh3\n",
    "import random\n",
    "import time\n",
    "# We will be using the bitarray module for the bloom filter because of the way Python handles lists\n",
    "# Elements of a Python list are objects and are therefore way larger than the bits we need to store\n",
    "from bitarray import bitarray"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the global variables\n",
    "sampled = False\n",
    "sr = 0.1 if sampled else 1.0\n",
    "genres_csv = \"data/genres.csv\"\n",
    "movies_csv = \"data/movies.csv\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Let's first define the bloom filter class since we have to re implement it from scratch\n",
    "class BloomFilter:\n",
    "    # The main parameters of a bloom filter are : the size of the bitmap and the number of hash functions used\n",
    "    def __init__(self, bitmap_size, hash_count):\n",
    "        self.size = bitmap_size\n",
    "        self.hash_count = hash_count\n",
    "        # Initialize the bitarray and initalize eveyrthing to 0\n",
    "        self.bit_array = bitarray(bitmap_size)\n",
    "        self.bit_array.setall(0)\n",
    "\n",
    "    # To add a value to the filter, we have to hash it multiple times and set the corresponding bits to 1\n",
    "    # Therefore we need to implement a hash function : in that case the hash function will directly return an array of all the indices to be set to 1\n",
    "    def _hashes(self, value):\n",
    "        hash_results = []\n",
    "        for i in range(self.hash_count):\n",
    "            # Once again we use the mmh3 hash function (here with 64 bits output) since it is faster than SHA\n",
    "            # We then take the modulo of the hash value to make sure it fits in the bitmap\n",
    "            # Note that the hash64 function returns a tuple with two 64 bit integers because it uses the same backend as the hash128, so we need to take only one\n",
    "            hash_result = mmh3.hash64(value, i)[0] % self.size\n",
    "            hash_results.append(hash_result)\n",
    "        return hash_results\n",
    "\n",
    "    # Adding a value to the filter is nothing but setting all the bits corresponding to the hash values to 1\n",
    "    def add(self, value):\n",
    "        for hash_val in self._hashes(value):\n",
    "            self.bit_array[hash_val] = 1\n",
    "\n",
    "    # Fianlly we need a function to \"query\" the filter, i.e. check if a value is in the filter\n",
    "    # To do this we need to check if all the bits corresponding to the hash values are 1\n",
    "    def query(self, value):\n",
    "        # We use the built in all() function that returns True if all the elements in the list in argument are True,\n",
    "        # and list comprehension to have a one liner for the check\n",
    "        return all(self.bit_array[hash_val] for hash_val in self._hashes(value))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now that we have defined the Bloom filter class, we can use it to our heart's contempt to filter what we want.\\\n",
    "In our case, we will process the genres file, add all the movie IDs that are associated with western, and then process the movie file which does not contain the genre information and try to categorize the movies.\\\n",
    "We can also try to apply an \"exact\" approach to estimate the performances of the filter."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Let's first define a helper function to process the file and \"fill up\" the bloom filter\n",
    "def init_filter(file, bloom):\n",
    "    start = time.process_time()\n",
    "    with open(file, 'r', encoding='utf-8') as csvfile:\n",
    "        csvreader = csv.DictReader(csvfile)\n",
    "        counter = 0\n",
    "        for row in csvreader:\n",
    "            # In that case we don't use the sample rate because we want ALL the western movies to be added to the filter\n",
    "            if row['genre'] == \"Western\":\n",
    "                bloom.add(row['id'])\n",
    "                counter += 1\n",
    "    print(f\"Added {counter} Western movies to the bloom filter\")\n",
    "    print(f\"Time taken to initialize the filter : {time.process_time() - start} seconds\")\n",
    "    return counter # We return the number of elements added to the filter for later use in the false positive rate calculation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Then we can define the main function that will test the bloom filter (process the movies file)\n",
    "def test_filter(file, bloom, sample_rate=1.0):\n",
    "    start = time.process_time()\n",
    "    with open(file, 'r', encoding='utf-8') as csvfile:\n",
    "        csvreader = csv.DictReader(csvfile)\n",
    "        counter = 0\n",
    "        ids = []\n",
    "        for row in csvreader:\n",
    "            # We use the sample rate to only check a fraction of the movies\n",
    "            if random.random() < sample_rate:\n",
    "                # If the movie id is in the filter, we increment the counter\n",
    "                if bloom.query(row['id']):\n",
    "                    counter += 1\n",
    "                    # We also keep track of the ids that have been found to check for false positives\n",
    "                    # This is only needed if the set is sampled because if it is not we can just compute the rate with the number found vs real number\n",
    "                    if sample_rate != 1.0:\n",
    "                        ids.append(row['id'])\n",
    "    print(f\"Found {counter} Western movies in the set\")\n",
    "    print(f\"Time taken to process the dataset : {time.process_time() - start} seconds\")\n",
    "    return counter, ids"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Let's also define a helper function to compute the number of false positive\n",
    "def count_fp(ids):\n",
    "    counter = 0\n",
    "    # We start by counting the number of movies that are actually western\n",
    "    with open(genres_csv, 'r', encoding='utf-8') as csvfile:\n",
    "        csvreader = csv.DictReader(csvfile)\n",
    "        for row in csvreader:\n",
    "            for id in ids:\n",
    "                if row['id'] == id and row['genre'] == \"Western\":\n",
    "                    counter += 1\n",
    "                    break\n",
    "    # Then we substract this number from the total number of movies found to get the number of false positives\n",
    "    return len(ids) - counter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Added 8638 Western movies to the bloom filter\n",
      "Time taken to initialize the filter : 1.609375 seconds\n",
      "Found 8638 Western movies in the set\n",
      "Time taken to process the dataset : 9.640625 seconds\n",
      "False positive rate : 0.0\n",
      "Memory usage of the filter : 62500.0 bytes\n"
     ]
    }
   ],
   "source": [
    "# Now that everything is defined, we can start the actual processing\n",
    "# First we initialize the filter, the size of the bitmap will determine the performance of the filter, depending on the number of elements to add\n",
    "bloom = BloomFilter(500000, 10)\n",
    "# Initialize the filter\n",
    "nb_westerns = init_filter(genres_csv, bloom)\n",
    "# Test the filter\n",
    "nb_found, ids = test_filter(movies_csv, bloom, sr)\n",
    "# Compute the false positive rate\n",
    "if not sampled:\n",
    "    # If the set is not sampled, we can directly compute the rate\n",
    "    # Since there cannot be false negatives, the number found is always >= to the real number\n",
    "    false_positives = nb_found - nb_westerns\n",
    "    false_positive_rate = false_positives / (nb_found)\n",
    "else:\n",
    "    # If the set is sampled, we have to check the ids that have been found and if they are really western movies\n",
    "    false_positives = count_fp(ids)\n",
    "    false_positive_rate = false_positives / len(ids)\n",
    "print(f\"False positive rate : {false_positive_rate}\")\n",
    "# Let's quickly compute the memory usage of the filter\n",
    "# Since we are dealing with a bitmap, each element is one bit, and we can just use the length of the bitmap to get the size in bits (converted in bytes afterwards)\n",
    "print(f\"Memory usage of the filter : {len(bloom.bit_array) / 8} bytes\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "After a first execution of the code, we can see that there are approximately 9000 western movies in the dataset. This information can help us to estimate how large the bitmap needs to be.\\\n",
    "In our case, we have 5 hash functions and 9000 movies, we want to have a bitmap that is quite sparse in order to avoid collisions as much as possible. If all the hash values are different, then we would get about 45000 ones in the bitmap, therefore choosing a bitmap size that is about 4/5 times bigger would already be a good start.\\\n",
    "With 200000 bits in the bitmap, we get a false positive rate of about 3% on the whole dataset, which is already pretty good. The bitmap is about 25kB and it takes less than 10 seconds to initialize the filter and process the dataset.\\\n",
    "The performances could be improved by adding more hash functions and increasing the size of the bitmap. Note that of course, the time to process the data as well as the memory needed are of course more important so there is a balance to find.\\\n",
    "With a bitmap of 500000 bits and 10 hash functions (about the same ratio as the previous), the false positive rate drops to 0, and it only takes 62kB of memory and about 10 seconds to process the whole dataset. Initializing the filter takes less than 2 seconds with this setup !"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
